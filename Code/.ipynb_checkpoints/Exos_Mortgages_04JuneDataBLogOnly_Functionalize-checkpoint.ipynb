{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Claira Project: Mortgage Contracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import string\n",
    "import numpy as np\n",
    "import gzip\n",
    "import seaborn as sns\n",
    "np.random.seed(99)\n",
    "RANDOM_STATE = 99\n",
    "import datetime\n",
    "pd.set_option('display.max_rows', 151)\n",
    "\n",
    "\n",
    "# Import vectorizing and modeling tools in preparation for modeling steps\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.feature_extraction import text, stop_words\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import time\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import spacy\n",
    "import en_core_web_lg\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "nlp = spacy.load('en_core_web_sm', disable = ['ner', 'parser'])\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, cross_val_predict\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Import CountVectorizer and TFIDFVectorizer from feature_extraction.text.\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing functions\n",
    "def explore_data_cleaning(raw_df):\n",
    "    trigger_order = raw_df['Trigger'].value_counts().sort_values(ascending = False).index\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(8, 8)\n",
    "    ax = sns.countplot(y=raw_df['Trigger'], data=raw_df, orient = 'h', order = trigger_order)\n",
    "    ax.set_title('Number of Trigger Types');\n",
    "\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(int(p.get_width()),((p.get_x() + p.get_width()), p.get_y()), xytext=(17, -15),fontsize=9,textcoords='offset points', horizontalalignment='right')\n",
    "\n",
    "    #https://stackoverflow.com/questions/50190409/how-to-annotate-horizontal-seaborn-countplots\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get data \n",
    "def get_data():\n",
    "    # Import the CSV file\n",
    "    raw_df = pd.read_csv('../data/sasb_cash_trap_triggers_06_04_20.csv')\n",
    "    return raw_df\n",
    "\n",
    "# function to clean data\n",
    "def clean_data(raw_df, print_fns=False):\n",
    "    # project out extra columns\n",
    "    raw_df = raw_df[['Document', 'Sentence', 'Trigger', 'Multiclass']]\n",
    "    \n",
    "    # Convert each category to Title format (to remove discrepancies based on capitalization)\n",
    "    raw_df['Trigger'] = raw_df['Trigger'].str.title()\n",
    "\n",
    "    # Some data fixes\n",
    "    raw_df['Trigger'] = raw_df['Trigger'].str.replace('Falll', 'Fall')\n",
    "    raw_df.loc[raw_df['Trigger'] == 'Aggregate Debt Yield', 'Trigger'] = 'Aggregate Debt Yield Fall'\n",
    "\n",
    "    # Some dollar sign symbols are causing problems - will remove them here\n",
    "    raw_df['Sentence'] = raw_df['Sentence'].replace({'\\$':''}, regex = True)\n",
    "\n",
    "    # Drop Multiclass column since it isn't needed\n",
    "    raw_df = raw_df.drop('Multiclass', axis = 1)\n",
    "    \n",
    "    # Count the number of distinct documents\n",
    "    num_docs = raw_df['Document'].nunique()\n",
    "    print(f'{num_docs} unique documents have been included in the data set')\n",
    "    \n",
    "    # Summarize the trigger counts for the existing Document set\n",
    "    max_triggers = raw_df['Document'].value_counts().max()\n",
    "    min_triggers = raw_df['Document'].value_counts().min()\n",
    "    print(f'The {num_docs} Documents have tag counts that range from {min_triggers} to {max_triggers}.')\n",
    "\n",
    "    # run the extra code that analyzes the data\n",
    "    if print_fns:\n",
    "        explore_data_cleaning(raw_df)\n",
    "        \n",
    "    return raw_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to reshape the dataframe such that the triggers are 0/1 columns. Remove duplicates\n",
    "def reshape_trigger_representation(dataframe):\n",
    "    # select trigger types\n",
    "    trigger_types = list(dataframe['Trigger'].unique())\n",
    "    trigger_types = [x for x in trigger_types if str(x) != 'nan']\n",
    "\n",
    "    # create new dataframe with unique document-sentence pairs (no duplicates)\n",
    "    reshaped = dataframe.drop('Trigger', axis = 1).drop_duplicates().reset_index().drop('index', axis = 1)\n",
    "\n",
    "    # select rows by trigger, reassign as 1 or 0 (for True or False)\n",
    "    for tt in trigger_types:\n",
    "        # select part that is trigger_type\n",
    "        temp = dataframe.loc[dataframe['Trigger'] == tt].copy()\n",
    "\n",
    "        # create a new column of true with trigger name\n",
    "        temp[tt.lower().replace(' ', '_')] = np.int64(1)\n",
    "        temp.drop('Trigger', axis=1, inplace=True)\n",
    "        temp[tt.lower().replace(' ', '_')] = temp[tt.lower().replace(' ', '_')].astype('Int64')\n",
    "\n",
    "        # left join this adjusted column to the unique data\n",
    "        reshaped = reshaped.merge(temp, how='left', left_on=['Document', 'Sentence'], right_on=['Document', 'Sentence'])\n",
    "\n",
    "    # replace nulls with False\n",
    "    reshaped = reshaped.fillna(0)  \n",
    "    \n",
    "    return reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shall we create doc view and remove the extraneous document without tags?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP processing the sentence columns to prep for models\n",
    "def tokenize_sentences(dataframe):\n",
    "    print('Tokenizing...')\n",
    "    dataframe['SentenceTokens'] = dataframe['Sentence'].apply(tokenize)\n",
    "    dataframe['SentenceLemmas'] = dataframe['Sentence'].apply(lemmatize)\n",
    "    print(f\"{len(dataframe['Sentence'])} sentences have been tokenized and lemmatized.\")\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incorporate Stopwords\n",
    "def get_stopwords():\n",
    "    # might need space\n",
    "    short_stopwords = ['the', 'to', 'of', 'be', 'and', 'in', 'a']\n",
    "    short_stopwords2 = ['the', 'and', 'a', 'to', 'it', 'be', 'for', 'with', 'that']\n",
    "    stopwords = list(STOP_WORDS)\n",
    "\n",
    "    return short_stopwords, short_stopwords2, stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to split data for each target column (trigger type)\n",
    "def get_split_data(df, target_info):\n",
    "    # extract target name\n",
    "    target = target_info['target']\n",
    "    x_col  = target_info['x_col']\n",
    "    \n",
    "    # create X, Y\n",
    "    X = df[x_col]\n",
    "    y = df[target]\n",
    "    indices = df.index\n",
    "\n",
    "    y = y.astype('int')\n",
    "\n",
    "    # run test, train split\n",
    "    X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(X, y, indices, test_size = 0.3, stratify = y, random_state = RANDOM_STATE)\n",
    "\n",
    "    # print information\n",
    "    print(f'X_train and y_train shapes: {X_train.shape}')\n",
    "    print(f'X_test and y_test shapes: {X_test.shape}')\n",
    "#    print(indices_train.shape, indices_test.shape)\n",
    "    \n",
    "    # create output dictionary\n",
    "    split_data = {}\n",
    "    split_data['X_train'] = X_train\n",
    "    split_data['X_test'] = X_test\n",
    "    split_data['y_train'] = y_train\n",
    "    split_data['y_test'] = y_test\n",
    "    split_data['indices_train'] = indices_train\n",
    "    split_data['indices_test'] = indices_test\n",
    "    \n",
    "    return split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the model with the chosen model and metric\n",
    "def run_model(split_data, target_info):\n",
    "\n",
    "    # get stopwords\n",
    "    short_stopwords, short_stopwords2, stopwords = get_stopwords()\n",
    "    \n",
    "    # set pipeline according to the selected model:\n",
    "    if target_info['model'] == 'lr':\n",
    "        # Define CVEC + Logistic Regression Pipeline\n",
    "        pipe = Pipeline([('cvec', CountVectorizer()), ('lr', LogisticRegression(solver = 'liblinear', random_state = RANDOM_STATE))])\n",
    "        params = {\n",
    "            'cvec__ngram_range': [(1,2), (1,3), (1,4), (1,5), (1,6), (1,7), (1,8)],\n",
    "            'cvec__stop_words': [None, short_stopwords, short_stopwords2, stopwords],  \n",
    "            'cvec__max_features': [100, 200, 400, 600, 1000],\n",
    "            'cvec__min_df': [2],\n",
    "            'cvec__max_df': [.99],\n",
    "            }\n",
    "\n",
    "    elif target_info['model'] == 'rf':\n",
    "        # Define CVEC + Logistic Regression Pipeline\n",
    "        pipe = Pipeline([('cvec', CountVectorizer()), ('rf', RandomForestClassifier(random_state = RANDOM_STATE, n_jobs = 2))])\n",
    "        params = {\n",
    "            'cvec__ngram_range': [(1,2), (1,3), (1,4), (1,5)],\n",
    "            'cvec__stop_words': [None, short_stopwords, short_stopwords2],  \n",
    "            'cvec__max_features': [100, 200, 400, 800],\n",
    "            'cvec__min_df': [2],\n",
    "            'cvec__max_df': [.99],\n",
    "            'rf__max_depth': [4,5, 6],\n",
    "            'rf__min_samples_split': [2,3],\n",
    "            'rf__min_samples_leaf': [10, 12]\n",
    "            }\n",
    "\n",
    "    \n",
    "    else:\n",
    "        print('did not specify model throw errrorrrrorrror')\n",
    "\n",
    "    # define pipeline\n",
    "    gs_model = GridSearchCV(pipe, param_grid = params, cv = 3, scoring = target_info['metric'])\n",
    "\n",
    "    # Start the timer.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # extract x/y_train\n",
    "    X_train = split_data['X_train']\n",
    "    y_train = split_data['y_train']\n",
    "    \n",
    "    # run pipeline\n",
    "    model_result = gs_model.fit(X_train, y_train)\n",
    "\n",
    "    print(f'Seconds elapsed for fitting: {(time.time() - t0):.3f}') # How many seconds elapsed.   \n",
    "    return model_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 unique documents have been included in the data set\n",
      "The 22 Documents have tag counts that range from 25 to 176.\n",
      "Tokenizing...\n",
      "1866 sentences have been tokenized and lemmatized.\n",
      "X_train and y_train shapes: (1306,)\n",
      "X_test and y_test shapes: (560,)\n",
      "Seconds elapsed for fitting: 192.647\n",
      "Loan Default train score: 0.987\n",
      "Loan Default test score: 0.915\n",
      "\n",
      "\n",
      "X_train and y_train shapes: (1306,)\n",
      "X_test and y_test shapes: (560,)\n"
     ]
    }
   ],
   "source": [
    "# Control of the main project\n",
    "\n",
    "# define dictionary of targets contains: tag, model, metric, input, order\n",
    "target_dict = {}\n",
    "target_dict['loan_default'] = {'target': 'loan_default', 'model': 'lr', 'metric':'f1', 'x_col':'Sentence'}\n",
    "target_dict['unspecified'] = {'target': 'unspecified', 'model': 'lr', 'metric':'f1', 'x_col':'Sentence'}\n",
    "target_dict['nontrigger'] = {'target': 'nontrigger', 'model': 'lr', 'metric': 'f1', 'x_col': 'Sentence'}\n",
    "\n",
    "# set output_dict - will contain target + output of calculations\n",
    "output_dict = {}\n",
    "\n",
    "# get data\n",
    "raw_df = get_data()\n",
    "raw_df = clean_data(raw_df)\n",
    "\n",
    "# Reshape our original dataframe and tokenize to prepare sentences for models\n",
    "df = reshape_trigger_representation(raw_df)\n",
    "df = tokenize_sentences(df)\n",
    "\n",
    "# run for each model definition\n",
    "for k,v in target_dict.items():\n",
    "    \n",
    "    # get split data\n",
    "    split_data = get_split_data(df, v)\n",
    "    \n",
    "    # run model\n",
    "    model_result = run_model(split_data, v)\n",
    "    \n",
    "    # make the output dictionary\n",
    "    output_dict[k] = v\n",
    "    output_dict[k]['split_data'] = split_data\n",
    "    output_dict[k]['model_result'] = model_result\n",
    "    \n",
    "    print(f\"{target_dict[k]['target'].replace('_', ' ').title()} train score: {model_result.score(split_data['X_train'], split_data['y_train']):.3f}\")\n",
    "    print(f\"{target_dict[k]['target'].replace('_', ' ').title()} test score: {model_result.score(split_data['X_test'], split_data['y_test']):.3f}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1060    “PIP Work” means any work to be performed purs...\n",
       "9       On each Loan Payment Date, during the continua...\n",
       "310     “Cash Sweep Event” means the occurrence of (i)...\n",
       "386     Capital Expenditures Reserve: 1/12th of 0.25 p...\n",
       "1263    “Bankruptcy Action” means with respect to any ...\n",
       "                              ...                        \n",
       "1019    Springing reserves for taxes, insurance, groun...\n",
       "761     Following the occurrence of a Debt Yield Trigg...\n",
       "1292    Within thirty (30) days after the end of each ...\n",
       "807     Following the occurrence of a Debt Yield Trigg...\n",
       "442     Under the terms of the Mortgage Loan Agreement...\n",
       "Name: Sentence, Length: 1306, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output_dict['loan_default']['split_data']['X_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loan Default train score: 0.987\n"
     ]
    }
   ],
   "source": [
    "# print(f\"{target_dict[k]['target'].replace('_', ' ').title()} train score: {model_result.score(split_data['X_train'], split_data['y_train']):.3f}\")\n",
    "# print(f' Train Score: {model_result.score(split_data[\"X_train\"], split_data[\"y_train\"])}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the CSV file\n",
    "# raw_df = pd.read_csv('../data/sasb_cash_trap_triggers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the CSV file\n",
    "raw_df = pd.read_csv('../data/sasb_cash_trap_triggers_06_04_20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the first few rows of the data set\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check size of the data set\n",
    "raw_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = raw_df[['Document', 'Sentence', 'Trigger', 'Multiclass']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "raw_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.dropna()\n",
    "raw_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review number of Trigger types\n",
    "raw_df['Trigger'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each category to Title format (to remove discrepancies based on capitalization)\n",
    "raw_df['Trigger'] = raw_df['Trigger'].str.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some data fixes\n",
    "raw_df['Trigger'] = raw_df['Trigger'].str.replace('Falll', 'Fall')\n",
    "raw_df.loc[raw_df['Trigger'] == 'Aggregate Debt Yield', 'Trigger'] = 'Aggregate Debt Yield Fall'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some dollar sign symbols are causing problems - will remove them here\n",
    "raw_df['Sentence'] = raw_df['Sentence'].replace({'\\$':''}, regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review clean Trigger Types\n",
    "raw_df['Trigger'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Multiclass column since it isn't needed\n",
    "raw_df = raw_df.drop('Multiclass', axis = 1)\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of distinct documents\n",
    "num_docs = raw_df['Document'].nunique()\n",
    "print(f'{num_docs} unique documents have been included in the data set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Review the distinct documents and the number of Trigger events identified for each\n",
    "raw_df['Document'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the trigger counts for the existing Document set\n",
    "max_triggers = raw_df['Document'].value_counts().max()\n",
    "min_triggers = raw_df['Document'].value_counts().min()\n",
    "print(f'The {num_docs} Documents have tag counts that range from {min_triggers} to {max_triggers}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review new number of Trigger types\n",
    "raw_df['Trigger'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigger_order = raw_df['Trigger'].value_counts().sort_values(ascending = False).index\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(8, 8)\n",
    "ax = sns.countplot(y=raw_df['Trigger'], data=raw_df, orient = 'h', order = trigger_order)\n",
    "ax.set_title('Number of Trigger Types');\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(int(p.get_width()),((p.get_x() + p.get_width()), p.get_y()), xytext=(17, -15),fontsize=9,textcoords='offset points', horizontalalignment='right')\n",
    "    \n",
    "#https://stackoverflow.com/questions/50190409/how-to-annotate-horizontal-seaborn-countplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review new distinct documents and the number of Trigger events identified for each\n",
    "raw_df['Document'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll reshape the data, such that each sentence is represented once, with separate categorization columns for each Trigger type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape our original dataframe\n",
    "df = reshape_trigger_representation(raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export reshaped to CSV file\n",
    "#df.to_csv('../data/reshaped_06_04_20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of tag types (for later modeling)\n",
    "trigger_list = list(df.columns)\n",
    "trigger_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Document' in trigger_list: trigger_list.remove('Document')\n",
    "if 'Sentence' in trigger_list: trigger_list.remove('Sentence')\n",
    "trigger_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe showing the document tagging details (removing sentence details)\n",
    "doc_view = df.groupby('Document').sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_view.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_tag_types = doc_view.astype(bool).sum(axis=1)\n",
    "number_tag_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The number of tags per document ranges from {number_tag_types.min()} to {number_tag_types.max()}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sum column\n",
    "doc_view['sum'] = doc_view.sum(axis = 1)\n",
    "doc_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export reshaped to CSV file\n",
    "#doc_view.to_csv('../data/doc_view_06_04_20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate documents that don't have any Trigger tags (only nontriggers)\n",
    "no_trigger_tags = list(doc_view.loc[doc_view['sum'] == doc_view['nontrigger']].index)\n",
    "no_trigger_tags # Save document names to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will remove any documents that have 0 Cash Trap Trigger clauses within the data set\n",
    "\n",
    "to_remove = df[df['Document'].isin(no_trigger_tags)]\n",
    "df = df.drop(to_remove.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_view.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing / Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column with length of Sentence\n",
    "df['sentence_char_count'] = df['Sentence'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable = ['ner', 'parser'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review a tokenized sample Sentence\n",
    "[token.text for token in nlp(df['Sentence'][2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review a lemmatized sample Sentence\n",
    "[token.lemma_ for token in nlp(df['Sentence'][2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['SentenceTokens'] = df['Sentence'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "df['SentenceLemmas'] = df['Sentence'].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all sentences into a list of review tokens\n",
    "all_sentence_tokens = ' '.join(df['SentenceTokens'])\n",
    "all_sentence_tokens[:75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all sentences into a list of review lemmas\n",
    "all_sentence_lemmas = ' '.join(df['SentenceLemmas'])\n",
    "all_sentence_lemmas[:75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_token_list = [token for token in all_sentence_tokens.split(' ')]\n",
    "sentence_lemma_list = [lemma for lemma in all_sentence_lemmas.split(' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_df = pd.Series(sentence_token_list)\n",
    "lemma_df = pd.Series(sentence_lemma_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the raw token count\n",
    "token_series = token_df.value_counts().head(15).sort_values(ascending = True)\n",
    "ax = token_series.plot.barh(figsize = (6,6))\n",
    "ax.set_xlabel('count')\n",
    "ax.set_ylabel('token')\n",
    "ax.set_title('Sentences: Raw Token Count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the raw lemma count\n",
    "lemma_series = lemma_df.value_counts().head(15).sort_values(ascending = True)\n",
    "ax = lemma_series.plot.barh(figsize = (6,6))\n",
    "ax.set_xlabel('count')\n",
    "ax.set_ylabel('lemma')\n",
    "ax.set_title('Sentences: Raw Lemma Count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation, maintain letters and numbers\n",
    "df['SentenceLemmas_nopunc'] = df['SentenceLemmas'].str.replace(\"[^a-zA-Z0-9#']\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all SentenceLemmas_nopunc into a list\n",
    "all_lemmas_nopunc = ' '.join(df['SentenceLemmas_nopunc'])\n",
    "lemma_list_nopunc = [lemma for lemma in all_lemmas_nopunc.split(' ')]\n",
    "lemma_list_nopunc = [lemma for lemma in lemma_list_nopunc if lemma not in ['']]\n",
    "lemma_list_nopunc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data series containing the list of words\n",
    "lemma_df_nopunc = pd.Series(lemma_list_nopunc)\n",
    "lemma_series_nopunc = lemma_df_nopunc.value_counts().head(15).sort_values(ascending = True)\n",
    "ax = lemma_series_nopunc.plot.barh(figsize = (6,6))\n",
    "ax.set_xlabel('count')\n",
    "ax.set_ylabel('lemma')\n",
    "ax.set_title('Sentences: Lemma Count, no punctuation');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SentenceLemmas_nopunc'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SentenceLemmas_nopunc'] = df['SentenceLemmas_nopunc'].str.replace('\\s{2,}', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SentenceLemmas_nopunc'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Preparation for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_stopwords = ['the', 'to', 'of', 'be', 'and', 'in', 'a']\n",
    "short_stopwords2 = ['the', 'and', 'a', 'to', 'it', 'be', 'for', 'with', 'that']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at original stopword list\n",
    "stopwords = list(STOP_WORDS)\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check our previously defined trigger list\n",
    "trigger_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_test_split(dataframe_sentence_column, trigger_list):\n",
    "    for trigger_type in trigger_list:\n",
    "        target = trigger_type\n",
    "        X = dataframe_sentence_column\n",
    "        y=df[target]\n",
    "        indices = df.index\n",
    "        y = y.astype('int')\n",
    "        X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(X, y, indices, test_size = 0.3, stratify = y, random_state = RANDOM_STATE)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by predicting Loan Default trigger\n",
    "\n",
    "#target = 'loan_default'\n",
    "\n",
    "\n",
    "#X = df['Sentence']\n",
    "#y = df[target]\n",
    "#indices = df.index\n",
    "\n",
    "#y = y.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking shape of X and y \n",
    "#print(f'X shape is {X.shape}')\n",
    "#print(f'y shape is {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(X, y, indices, test_size = 0.3, stratify = y, random_state = RANDOM_STATE)\n",
    "#print(X_train.shape, y_train.shape)\n",
    "#print(X_test.shape, y_test.shape)\n",
    "#print(indices_train.shape, indices_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modeling_function(target):\n",
    "    pipe_cvec_lr = Pipeline([('cvec', CountVectorizer()), ('lr', LogisticRegression(solver = 'liblinear', random_state = RANDOM_STATE))])\n",
    "    lr_params = {\n",
    "        'lr__ngram_range': [(1,2), (1,3), (1,4), (1,5), (1,6), (1,7), (1,8)],\n",
    "        'lr__stop_words': [None, short_stopwords, short_stopwords2, stopwords],  \n",
    "        'lr__max_features': [100, 200, 400, 600, 1000],\n",
    "        'lr__min_df': [2],\n",
    "        'lr__max_df': [.99],\n",
    "        }\n",
    "\n",
    "    gs_lr = GridSearchCV(pipe_cvec_lr, param_grid = lr_params, cv = 3, scoring = 'f1')\n",
    "\n",
    "    # Start the timer.\n",
    "    t0 = time.time()\n",
    "\n",
    "    results_lr = gs_lr.fit(X_train, y_train)\n",
    "\n",
    "    print(f'Seconds elapsed for fitting: {(time.time() - t0):.3f}') # How many seconds elapsed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer + Logistic Regression GridSearch and modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CVEC + Logistic Regression Pipeline\n",
    "pipe_cvec = Pipeline([('cvec', CountVectorizer()), ('lr', LogisticRegression(solver = 'liblinear', random_state = RANDOM_STATE))])\n",
    "cvec_params = {\n",
    "    'cvec__ngram_range': [(1,2), (1,3), (1,4), (1,5), (1,6), (1,7), (1,8)],\n",
    "    'cvec__stop_words': [None, short_stopwords, short_stopwords2, stopwords],  \n",
    "    'cvec__max_features': [100, 200, 400, 600, 1000],\n",
    "    'cvec__min_df': [2],\n",
    "    'cvec__max_df': [.99],\n",
    "    }\n",
    "\n",
    "gs_cvec = GridSearchCV(pipe_cvec, param_grid = cvec_params, cv = 3, scoring = 'f1')\n",
    "\n",
    "# Start the timer.\n",
    "t0 = time.time()\n",
    "\n",
    "results_cvec = gs_cvec.fit(X_train, y_train)\n",
    "\n",
    "print(f'Seconds elapsed for fitting: {(time.time() - t0):.3f}') # How many seconds elapsed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "print(f'Training score is {results_cvec.score(X_train, y_train):.3f}')\n",
    "print(f'Test score is {results_cvec.score(X_test, y_test):.3f}')\n",
    "print(f'Cross Validation score is {cross_val_score(results_cvec.best_estimator_, X, y, cv = 3).mean():.3f}')\n",
    "print(f'Seconds elapsed for score calculation: {(time.time() - t0):.3f}') # How many seconds elapsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Best Score:{(results_cvec.best_score_):.3f}')\n",
    "print(f'Best Parameters :{results_cvec.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5: CountVectorizor + Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Define CVEC + Logistic Regression Pipeline\n",
    "pipe_rf = Pipeline([('cvec', CountVectorizer()), ('rf', RandomForestClassifier(random_state = RANDOM_STATE, n_jobs = 2))])\n",
    "rf_params = {\n",
    "    'cvec__ngram_range': [(1,2), (1,3), (1,4), (1,5)],\n",
    "    'cvec__stop_words': [None, short_stopwords, short_stopwords2],  \n",
    "    'cvec__max_features': [100, 200, 400, 800],\n",
    "    'cvec__min_df': [2],\n",
    "    'cvec__max_df': [.99],\n",
    "    'rf__max_depth': [4,5, 6],\n",
    "    'rf__min_samples_split': [2,3],\n",
    "    'rf__min_samples_leaf': [10, 12]\n",
    "    }\n",
    "\n",
    "gs_rf = GridSearchCV(pipe_rf, param_grid = rf_params, cv = 3, scoring = 'f1')\n",
    "\n",
    "# Start the timer.\n",
    "t0 = time.time()\n",
    "\n",
    "results_rf = gs_rf.fit(X_train, y_train)\n",
    "\n",
    "print(f'Seconds elapsed for fitting: {(time.time() - t0):.3f}') # How many seconds elapsed.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "t0 = time.time()\n",
    "print(f'Training score is {results_rf.score(X_train, y_train):.3f}')\n",
    "print(f'Test score is {results_rf.score(X_test, y_test):.3f}')\n",
    "print(f'Cross Validation score is {cross_val_score(results_rf.best_estimator_, X, y, cv = 3).mean():.3f}')\n",
    "print(f'Seconds elapsed for score calculation: {(time.time() - t0):.3f}') # How many seconds elapsed.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "print(f'Best Score: {results_rf.best_score_}')\n",
    "print(f'Best Parameters: {results_rf.best_params_}')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "- In this section we compare the Train and Test scores across the various models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Training Scores across all models\n",
    "print(f'Baseline Score:                                {y_test.value_counts(normalize=True)[0]:.3f}')\n",
    "print(f'CountVectorizer + LogisticRegression Accuracy: {results_cvec.score(X_train, y_train):.3f}')\n",
    "# print(f'CountVectorizer + Random Forest Accuracy:      {results_rf.score(X_train, y_train):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Testing Scores across all models\n",
    "print(f'Baseline Score:                                {y_test.value_counts(normalize=True)[0]:.3f}')\n",
    "print(f'CountVectorizer + LogisticRegression Accuracy: {results_cvec.score(X_test, y_test):.3f}')\n",
    "# print(f'CountVectorizer + Random Forest Accuracy:      {results_rf.score(X_test, y_test):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate the individual words and their coefficients\n",
    "# Feature names (i.e., words in the Sentences):\n",
    "names = results_cvec.best_estimator_.steps[0][1].get_feature_names()\n",
    "\n",
    "# classifier (betas):\n",
    "classifier = results_cvec.best_estimator_.named_steps['lr']\n",
    "\n",
    "# https://stackoverflow.com/questions/43856280/return-coefficients-from-pipeline-object-in-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coefficients from the classifier defined above\n",
    "coef_cvec = np.array(classifier.coef_).tolist()[0]\n",
    "coef_cvec[:10] #Look at 10 coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create zipped list of the word names with their corresponding beta coefficients\n",
    "cvec_top_words = list(zip(names, coef_cvec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final dataframe of words with their corresponding coefficients\n",
    "df_cvec_coefs = pd.DataFrame(cvec_top_words).rename(columns = {0: 'word', 1: 'coef'}).sort_values(by = 'coef', ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top differentiating words and phrases for this Trigger type\n",
    "df_cvec_coefs.tail(20).sort_values('coef', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top opposing words and phrases for this Trigger type\n",
    "df_cvec_coefs.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coef_plot(category):\n",
    "    '''\n",
    "    Plots the top 10 and bottom 10 coefficients for the complexity category specified\n",
    "    '''\n",
    "    coefs_1 = df_cvec_coefs.sort_values(by=category, ascending=False).tail(10) # getting the top 10 coefficients\n",
    "    coefs_2 = df_cvec_coefs.sort_values(by=category, ascending=False).head(10) # getting the bottom 10 coefficients\n",
    "    coefs = pd.concat([coefs_2, coefs_1], axis = 0) # merging the two into one\n",
    "    # plotting importance\n",
    "    plt.figure(figsize=(10, 8)) # plotting the coefficients\n",
    "    plt.title(f'Feature Coefficients for {target.replace(\"_\", \" \").title()}', fontsize=25)\n",
    "    sns.set_style(\"darkgrid\")\n",
    "    sns.barplot(data=coefs,\n",
    "                x=category,\n",
    "                y='word',\n",
    "                orient='h',\n",
    "                palette = 'PuBuGn_d')\n",
    "    plt.xlabel('coefficient', fontsize=15)\n",
    "    plt.ylabel('feature', fontsize=15)\n",
    "    plt.tick_params(labelsize=15)\n",
    "coef_plot('coef')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating predictions and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we isolate our best model & make predictions based on our test data\n",
    "best_model = results_cvec.best_estimator_\n",
    "preds = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking our predictions\n",
    "preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm_test = confusion_matrix(y_test, preds)\n",
    "print('This is a confusion matrix for our test data vs predictions:')\n",
    "print(cm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting our confusion matrix into a dataframe\n",
    "cm_test = pd.DataFrame(cm_test, columns=['Predicted Negative','Predicted Positive'], \n",
    "                       index=['Actual Negative','Actual Positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot our confusion matrix\n",
    "plt.figure(figsize = (6,6))\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(cm_test, \n",
    "            annot=True, \n",
    "            ax = ax, \n",
    "            fmt='g', \n",
    "            cbar=False,\n",
    "            cmap=\"Blues\"); #annot=True to annotate cells\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_title('Confusion Matrix - Trigger Prediction', size=16)\n",
    "ax.set_xlabel('Predicted', size=14)\n",
    "ax.set_ylabel('Actual', size=14)\n",
    "ax.xaxis.set_ticklabels(['Negative', 'Positive'])\n",
    "ax.yaxis.set_ticklabels(['Negative', 'Positive']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up probabilities for the ROC curve\n",
    "pred_proba = results_cvec.predict_proba(X_test)\n",
    "preds = results_cvec.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _= roc_curve(y_test, pred_proba[:,1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "# Plot of a ROC curve for a specific class\n",
    "plt.figure(figsize = (8,8))\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.4f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='baseline')\n",
    "plt.xlim([-0.01, 1.0])\n",
    "plt.ylim([0.0, 1.01])\n",
    "plt.xlabel('False Positive Rate', fontsize =20)\n",
    "plt.ylabel('True Positive Rate', fontsize = 20)\n",
    "plt.title('Receiver Operating Characteristic Curve', fontsize=18)\n",
    "plt.legend(loc=\"lower right\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To allow us to better read the Sentences\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_cols = ['index', 'prediction', 'actual', 'model_input']\n",
    "results = pd.DataFrame({'index': list(indices_test),'prediction': list(preds), 'actual': list(y_test), 'model_input': list(X_test)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set index as index column\n",
    "results.set_index('index', inplace = True)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified = results[results['prediction'] != results['actual']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified = misclassified.merge(df, how = 'left', left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified = misclassified[['prediction', 'actual', 'model_input', 'Document', 'Sentence',\n",
    "       'loan_default', 'aggregate_dscr_fall', 'dscr_fall', 'unspecified',\n",
    "       'debt_yield_fall', 'aggregate_debt_yield_fall', 'mezzanine_default',\n",
    "       'tenant_failure', 'mezzanine_outstanding', 'operator_termination',\n",
    "       'bankruptcy', 'sponsor_termination', 'renovations', 'nontrigger']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# misclassified['prediction'].astype('Int64')\n",
    "# misclassified['actual'].astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'We incorrectly predicted the trigger type for {misclassified.shape[0]} sentences.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the split of the misclassified predictions\n",
    "misclassified['prediction'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review the Incorrect Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misclassifications: wrongly predicted to be the trigger category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified_as_true = misclassified.loc[misclassified['prediction'] == 1]\n",
    "misclassified_as_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified_as_true_summary = misclassified_as_true[['prediction', 'actual', 'loan_default', 'aggregate_dscr_fall', 'dscr_fall', 'unspecified',\n",
    "       'debt_yield_fall', 'aggregate_debt_yield_fall', 'mezzanine_default',\n",
    "       'tenant_failure', 'mezzanine_outstanding', 'operator_termination',\n",
    "       'bankruptcy', 'sponsor_termination', 'renovations', 'nontrigger']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified_as_true_count = misclassified_as_true_summary.shape[0]\n",
    "print(f'{misclassified_as_true_count} sentences were predicted to be the trigger category, but in fact were not this category.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "misclassified_as_true_summary.loc[f'{target}_sum',:] = misclassified_as_true.sum(axis = 0).copy(deep = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified_as_true_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misclassifications: wrongly predicted NOT to be the trigger category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified_as_false = misclassified.loc[misclassified['prediction'] == 0]\n",
    "misclassified_as_false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified_as_false_summary = misclassified_as_false[['prediction', 'actual', 'loan_default', 'aggregate_dscr_fall', 'dscr_fall', 'unspecified',\n",
    "       'debt_yield_fall', 'aggregate_debt_yield_fall', 'mezzanine_default',\n",
    "       'tenant_failure', 'mezzanine_outstanding', 'operator_termination',\n",
    "       'bankruptcy', 'sponsor_termination', 'renovations', 'nontrigger']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified_as_false_count = misclassified_as_false.shape[0]\n",
    "print(f'{misclassified_as_false_count} sentences were predicted not to be the trigger category, but in fact were this trigger category.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified_as_false_summary.loc[f'{target}_sum',:] = misclassified_as_false.sum(axis = 0).copy(deep = True)\n",
    "misclassified_as_false_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Misclassification summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV file\n",
    "#misclassified_as_false_summary.to_csv(f'../data/misclassified/{target}_misclassified_as_false_06_04_20.csv')\n",
    "#misclassified_as_true_summary.to_csv(f'../data/misclassified/{target}_misclassified_as_true_06_04_20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "415.764px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
