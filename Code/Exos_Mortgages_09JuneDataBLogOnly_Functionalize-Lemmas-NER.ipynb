{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Claira Project: Mortgage Contracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import string\n",
    "import numpy as np\n",
    "import gzip\n",
    "import seaborn as sns\n",
    "np.random.seed(99)\n",
    "RANDOM_STATE = 99\n",
    "import datetime\n",
    "pd.set_option('display.max_rows', 151)\n",
    "pd.set_option('display.max_columns', 30)\n",
    "\n",
    "\n",
    "# Import vectorizing and modeling tools in preparation for modeling steps\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.feature_extraction import text, stop_words\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import spacy\n",
    "import en_core_web_lg\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "nlp = spacy.load('en_core_web_lg', disable = 'parser')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, cross_val_predict\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Import CountVectorizer and TFIDFVectorizer from feature_extraction.text.\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define sub-functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing functions\n",
    "def explore_data_cleaning(raw_df):\n",
    "    trigger_order = raw_df['Trigger'].value_counts().sort_values(ascending = False).index\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(8, 8)\n",
    "    ax = sns.countplot(y=raw_df['Trigger'], data=raw_df, orient = 'h', order = trigger_order)\n",
    "    ax.set_title('Number of Trigger Types');\n",
    "\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(int(p.get_width()),((p.get_x() + p.get_width()), p.get_y()), xytext=(17, -15),fontsize=9,textcoords='offset points', horizontalalignment='right')\n",
    "\n",
    "    #https://stackoverflow.com/questions/50190409/how-to-annotate-horizontal-seaborn-countplots\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get data \n",
    "def get_data():\n",
    "    # Import the CSV file\n",
    "    raw_df = pd.read_csv('../data/sasb_cash_trap_triggers_06_09_20.csv')\n",
    "    return raw_df\n",
    "\n",
    "# function to clean data\n",
    "def clean_data(raw_df, print_fns=False):\n",
    "    # project out extra columns\n",
    "    raw_df = raw_df[['Document', 'Sentence', 'Trigger', 'Multiclass']]\n",
    "    \n",
    "    # Convert each category to Title format (to remove discrepancies based on capitalization)\n",
    "    raw_df['Trigger'] = raw_df['Trigger'].str.title()\n",
    "\n",
    "    # Some data fixes\n",
    "    raw_df['Trigger'] = raw_df['Trigger'].str.replace('Falll', 'Fall')\n",
    "    raw_df.loc[raw_df['Trigger'] == 'Aggregate Debt Yield', 'Trigger'] = 'Aggregate Debt Yield Fall'\n",
    "\n",
    "    # Some dollar sign symbols are causing problems - will remove them here\n",
    "    raw_df['Sentence'] = raw_df['Sentence'].replace({'\\$':''}, regex = True)\n",
    "\n",
    "    # Drop Multiclass column since it isn't needed\n",
    "    raw_df = raw_df.drop('Multiclass', axis = 1)\n",
    "    \n",
    "    # Count the number of distinct documents\n",
    "    num_docs = raw_df['Document'].nunique()\n",
    "    print(f'{num_docs} unique documents have been included in the data set')\n",
    "    \n",
    "    # Summarize the trigger counts for the existing Document set\n",
    "    max_triggers = raw_df['Document'].value_counts().max()\n",
    "    min_triggers = raw_df['Document'].value_counts().min()\n",
    "    print(f'The {num_docs} Documents have tag counts that range from {min_triggers} to {max_triggers}.')\n",
    "\n",
    "    # run the extra code that analyzes the data\n",
    "    if print_fns:\n",
    "        explore_data_cleaning(raw_df)\n",
    "        \n",
    "    return raw_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to reshape the dataframe such that the triggers are 0/1 columns. Remove duplicates\n",
    "def reshape_trigger_representation(dataframe):\n",
    "    # select trigger types\n",
    "    trigger_types = list(dataframe['Trigger'].unique())\n",
    "    trigger_types = [x for x in trigger_types if str(x) != 'nan']\n",
    "\n",
    "    # create new dataframe with unique document-sentence pairs (no duplicates)\n",
    "    reshaped = dataframe.drop('Trigger', axis = 1).drop_duplicates().reset_index().drop('index', axis = 1)\n",
    "\n",
    "    # select rows by trigger, reassign as 1 or 0 (for True or False)\n",
    "    for tt in trigger_types:\n",
    "        # select part that is trigger_type\n",
    "        temp = dataframe.loc[dataframe['Trigger'] == tt].copy()\n",
    "\n",
    "        # create a new column of true with trigger name\n",
    "        temp[tt.lower().replace(' ', '_')] = np.int64(1)\n",
    "        temp.drop('Trigger', axis=1, inplace=True)\n",
    "        temp[tt.lower().replace(' ', '_')] = temp[tt.lower().replace(' ', '_')].astype('Int64')\n",
    "\n",
    "        # left join this adjusted column to the unique data\n",
    "        reshaped = reshaped.merge(temp, how='left', left_on=['Document', 'Sentence'], right_on=['Document', 'Sentence'])\n",
    "\n",
    "    # replace nulls with False\n",
    "    reshaped = reshaped.fillna(0)  \n",
    "    \n",
    "    return reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shall we create doc view and remove the extraneous document without tags?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP processing the sentence columns to prep for models\n",
    "def tokenize_sentences(dataframe):\n",
    "    print('Tokenizing the input sentences...')\n",
    "    dataframe['SentenceTokens'] = dataframe['Sentence'].apply(tokenize)\n",
    "    dataframe['SentenceLemmas'] = dataframe['Sentence'].apply(lemmatize)\n",
    "    print(f\"{len(dataframe['Sentence'])} sentences have been tokenized and lemmatized.\")\n",
    "    print(\"\\n\")\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incorporate Stopwords\n",
    "def get_stopwords():\n",
    "    # might need space\n",
    "    short_stopwords = ['the', 'to', 'of', 'be', 'and', 'in', 'a']\n",
    "    short_stopwords2 = ['the', 'and', 'a', 'to', 'it', 'be', 'for', 'with', 'that']\n",
    "    stopwords = list(STOP_WORDS)\n",
    "\n",
    "    return short_stopwords, short_stopwords2, stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to split data for each target column (trigger type)\n",
    "def get_split_data(df, target_info):\n",
    "    # extract target name\n",
    "    target = target_info['target']\n",
    "    model_input  = target_info['model_input']\n",
    "    \n",
    "    # create X, Y\n",
    "    X = df[model_input]\n",
    "    y = df[target]\n",
    "    indices = df.index\n",
    "\n",
    "    print(f\"Number of labeled instances within the full sentence data set: {y.value_counts()[1]}\")   \n",
    "    \n",
    "    y = y.astype('int')\n",
    "\n",
    "    # run test, train split\n",
    "    X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(X, y, indices, test_size = 0.3, stratify = y, random_state = RANDOM_STATE)\n",
    "\n",
    "    \n",
    "    # create output dictionary\n",
    "    split_data = {}\n",
    "    split_data['X_train'] = X_train\n",
    "    split_data['X_test'] = X_test\n",
    "    split_data['y_train'] = y_train\n",
    "    split_data['y_test'] = y_test\n",
    "    split_data['indices_train'] = indices_train\n",
    "    split_data['indices_test'] = indices_test\n",
    "    \n",
    "    return split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the model with the chosen model and metric\n",
    "def run_model(split_data, target_info):\n",
    "\n",
    "    # get stopwords\n",
    "    short_stopwords, short_stopwords2, stopwords = get_stopwords()\n",
    "    \n",
    "    # set pipeline according to the selected model:\n",
    "    if target_info['model'] == 'lr':\n",
    "        # Define CVEC + Logistic Regression Pipeline\n",
    "        pipe = Pipeline([('cvec', CountVectorizer()), ('lr', LogisticRegression(solver = 'liblinear', random_state = RANDOM_STATE))])\n",
    "        params = {\n",
    "            'cvec__ngram_range': [(1,2), (1,3), (1,4), (1,5), (1,6), (1,7)],\n",
    "            'cvec__stop_words': [None, short_stopwords, short_stopwords2, stopwords],  \n",
    "            'cvec__max_features': [100, 200, 400, 600, 1000],\n",
    "            'cvec__min_df': [2],\n",
    "            'cvec__max_df': [.99],\n",
    "            }\n",
    "\n",
    "    elif target_info['model'] == 'rf':\n",
    "        # Define CVEC + Logistic Regression Pipeline\n",
    "        pipe = Pipeline([('cvec', CountVectorizer()), ('rf', RandomForestClassifier(random_state = RANDOM_STATE, n_jobs = 2))])\n",
    "        params = {\n",
    "            'cvec__ngram_range': [(1,2), (1,3), (1,4), (1,5)],\n",
    "            'cvec__stop_words': [None, short_stopwords, short_stopwords2],  \n",
    "            'cvec__max_features': [100, 200, 400, 800],\n",
    "            'cvec__min_df': [2],\n",
    "            'cvec__max_df': [.99],\n",
    "            'rf__max_depth': [4,5, 6],\n",
    "            'rf__min_samples_split': [2,3],\n",
    "            'rf__min_samples_leaf': [10, 12]\n",
    "            }\n",
    "\n",
    "    \n",
    "    else:\n",
    "        print('did not specify model throw error')\n",
    "\n",
    "    # define pipeline\n",
    "    gs_model = GridSearchCV(pipe, param_grid = params, cv = 3, scoring = target_info['metric'])\n",
    "\n",
    "    # Start the timer.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # extract X_train and y_train\n",
    "    X_train = split_data['X_train']\n",
    "    y_train = split_data['y_train']\n",
    "    \n",
    "    # run pipeline\n",
    "    model_result = gs_model.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"Seconds elapsed for fitting: {(time.time() - t0):.3f}\") # How many seconds elapsed.   \n",
    "    return model_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Main Control Function - runs the full model for all target types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 unique documents have been included in the data set\n",
      "The 27 Documents have tag counts that range from 25 to 176.\n",
      "Tokenizing the input sentences...\n",
      "2041 sentences have been tokenized and lemmatized.\n",
      "\n",
      "\n",
      "Nontrigger: creating Train-Test split\n",
      "Number of labeled instances within the full sentence data set: 1056\n",
      "Model fit in progress: {'target': 'nontrigger', 'model': 'lr', 'metric': 'accuracy', 'model_input': 'SentenceLemmas'}\n",
      "Seconds elapsed for fitting: 183.954\n",
      "Best fit parameters: {'cvec__max_df': 0.99, 'cvec__max_features': 1000, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 3), 'cvec__stop_words': ['back', 'do', 'nine', 'fifty', 'various', 'then', 'due', 'i', 'together', 'move', 'no', 'which', 'anyhow', 'less', 'regarding', 'me', 'yet', 'that', 'such', 'your', 'own', 'nevertheless', 'otherwise', 'became', 'cannot', 'most', 'put', 'whole', 'am', 'further', 'thus', 'even', 'down', 'have', 'over', 'really', 'becoming', 'as', 'five', 'is', 'someone', 'whereas', 'could', 'least', 'so', 'thereby', 'behind', 'between', 'be', 'by', 'else', 'already', 'mine', 'must', 'or', 'via', 'hereby', 'hers', 'other', 'more', 'somewhere', 'amongst', 'every', 'should', 'upon', 'below', 'neither', 'serious', 'if', 'perhaps', 'either', 'becomes', 'her', 'in', 'itself', 'off', 'anyone', 'noone', 'very', 'beyond', 'an', 'because', 'nor', 'ever', 'see', 'using', 'hundred', 'rather', 'whereafter', 'first', 'she', 'does', 'forty', 'seeming', 'us', 'alone', 'third', 'both', 'did', 'next', 'none', 'ten', 'herein', 'again', 'elsewhere', 'ourselves', 'he', 'per', 'than', 'only', 'sixty', 'those', 'to', 'whence', 'anything', 'moreover', 'say', 'well', 'whoever', 'until', 'besides', 'would', 'sometimes', 'almost', 'against', 'being', 'any', 'while', 'might', 'all', 'indeed', 'ours', 'they', 'call', 'thru', 'twenty', 'within', 'latterly', 'but', 'full', 'two', 'wherein', 'out', 'himself', 'however', 'on', 'make', 'used', 'without', 'everyone', 'once', 'beforehand', 'therefore', 'each', 'twelve', 'go', 'anyway', 'former', 'meanwhile', 'therein', 'four', 'last', 'made', 'side', 'sometime', 'mostly', 'yourself', 'a', 'nowhere', 'themselves', 'doing', 'too', 'you', 'often', 'the', 'still', 'him', 'six', 'their', 'show', 'though', 'for', 'few', 'whereupon', 'whose', 'everywhere', 'across', 'after', 're', 'keep', 'was', 'another', 'become', 'afterwards', 'can', 'thence', 'throughout', 'whether', 'his', 'yours', 'ca', 'done', 'and', 'under', 'beside', 'enough', 'several', 'who', 'will', 'everything', 'never', 'nothing', 'were', 'had', 'into', 'some', 'give', 'up', 'get', 'about', 'here', 'myself', 'where', 'hereafter', 'one', 'now', 'our', 'with', 'hereupon', 'yourselves', 'are', 'seemed', 'it', 'this', 'unless', 'not', 'at', 'bottom', 'namely', 'much', 'before', 'empty', 'has', 'latter', 'through', 'although', 'quite', 'whereby', 'there', 'whither', 'part', 'onto', 'around', 'of', 'from', 'many', 'them', 'why', 'something', 'top', 'eleven', 'eight', 'these', 'whenever', 'fifteen', 'how', 'somehow', 'always', 'thereupon', 'along', 'please', 'anywhere', 'seem', 'whom', 'thereafter', 'except', 'others', 'wherever', 'seems', 'nobody', 'my', 'among', 'formerly', 'during', 'name', 'take', 'above', 'may', 'same', 'three', 'been', 'also', 'just', 'toward', 'towards', 'what', 'amount', 'its', 'we', 'when', 'hence', 'herself', 'since', 'front', 'whatever']}\n",
      "Best fit 3-fold cross validation score: 0.971\n",
      "Nontrigger accuracy Train score: 0.992\n",
      "Nontrigger accuracy Test score: 0.974\n",
      "\n",
      "\n",
      "Loan Default: creating Train-Test split\n",
      "Number of labeled instances within the full sentence data set: 459\n",
      "Model fit in progress: {'target': 'loan_default', 'model': 'lr', 'metric': 'accuracy', 'model_input': 'SentenceLemmas'}\n",
      "Seconds elapsed for fitting: 179.488\n",
      "Best fit parameters: {'cvec__max_df': 0.99, 'cvec__max_features': 100, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 7), 'cvec__stop_words': ['the', 'to', 'of', 'be', 'and', 'in', 'a']}\n",
      "Best fit 3-fold cross validation score: 0.981\n",
      "Loan Default accuracy Train score: 0.991\n",
      "Loan Default accuracy Test score: 0.979\n",
      "\n",
      "\n",
      "Unspecified: creating Train-Test split\n",
      "Number of labeled instances within the full sentence data set: 356\n",
      "Model fit in progress: {'target': 'unspecified', 'model': 'lr', 'metric': 'accuracy', 'model_input': 'SentenceLemmas'}\n",
      "Seconds elapsed for fitting: 212.505\n",
      "Best fit parameters: {'cvec__max_df': 0.99, 'cvec__max_features': 1000, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': ['the', 'to', 'of', 'be', 'and', 'in', 'a']}\n",
      "Best fit 3-fold cross validation score: 0.957\n",
      "Unspecified accuracy Train score: 0.999\n",
      "Unspecified accuracy Test score: 0.959\n",
      "\n",
      "\n",
      "Debt Yield Fall: creating Train-Test split\n",
      "Number of labeled instances within the full sentence data set: 204\n",
      "Model fit in progress: {'target': 'debt_yield_fall', 'model': 'lr', 'metric': 'f1', 'model_input': 'SentenceLemmas'}\n",
      "Seconds elapsed for fitting: 219.354\n",
      "Best fit parameters: {'cvec__max_df': 0.99, 'cvec__max_features': 1000, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 4), 'cvec__stop_words': None}\n",
      "Best fit 3-fold cross validation score: 0.954\n",
      "Debt Yield Fall f1 Train score: 1.000\n",
      "Debt Yield Fall f1 Test score: 0.967\n",
      "\n",
      "\n",
      "Mezzanine Default: creating Train-Test split\n",
      "Number of labeled instances within the full sentence data set: 105\n",
      "Model fit in progress: {'target': 'mezzanine_default', 'model': 'lr', 'metric': 'f1', 'model_input': 'SentenceLemmas'}\n",
      "Seconds elapsed for fitting: 227.025\n",
      "Best fit parameters: {'cvec__max_df': 0.99, 'cvec__max_features': 100, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 3), 'cvec__stop_words': ['the', 'to', 'of', 'be', 'and', 'in', 'a']}\n",
      "Best fit 3-fold cross validation score: 0.965\n",
      "Mezzanine Default f1 Train score: 0.993\n",
      "Mezzanine Default f1 Test score: 0.918\n",
      "\n",
      "\n",
      "Tenant Failure: creating Train-Test split\n",
      "Number of labeled instances within the full sentence data set: 37\n",
      "Model fit in progress: {'target': 'tenant_failure', 'model': 'lr', 'metric': 'f1', 'model_input': 'SentenceLemmas'}\n"
     ]
    }
   ],
   "source": [
    "# Control of the main project\n",
    "\n",
    "# define dictionary of targets contains: tag, model, metric, input, order\n",
    "target_dict = {}\n",
    "target_dict['nontrigger'] = {'target': 'nontrigger', 'model': 'lr', 'metric': 'accuracy', 'model_input': 'SentenceLemmas'}\n",
    "target_dict['loan_default'] = {'target': 'loan_default', 'model': 'lr', 'metric':'accuracy', 'model_input':'SentenceLemmas'}\n",
    "target_dict['unspecified'] = {'target': 'unspecified', 'model': 'lr', 'metric':'accuracy', 'model_input':'SentenceLemmas'}\n",
    "target_dict['debt_yield_fall'] = {'target': 'debt_yield_fall', 'model': 'lr', 'metric': 'f1', 'model_input': 'SentenceLemmas'}\n",
    "target_dict['mezzanine_default'] = {'target': 'mezzanine_default', 'model': 'lr', 'metric': 'f1', 'model_input': 'SentenceLemmas'}\n",
    "target_dict['tenant_failure'] = {'target': 'tenant_failure', 'model': 'lr', 'metric': 'f1', 'model_input': 'SentenceLemmas'}\n",
    "target_dict['bankruptcy'] = {'target': 'bankruptcy', 'model': 'lr', 'metric': 'accuracy', 'model_input': 'SentenceLemmas'}\n",
    "target_dict['aggregate_debt_yield_fall'] = {'target': 'aggregate_debt_yield_fall', 'model': 'lr', 'metric': 'f1', 'model_input': 'SentenceLemmas'}\n",
    "target_dict['dscr_fall'] = {'target': 'dscr_fall', 'model': 'lr', 'metric': 'f1', 'model_input': 'SentenceLemmas'}\n",
    "target_dict['renovations'] = {'target': 'renovations', 'model': 'lr', 'metric': 'f1', 'model_input': 'SentenceLemmas'}\n",
    "target_dict['operator_termination'] = {'target': 'operator_termination', 'model': 'lr', 'metric': 'f1', 'model_input': 'SentenceLemmas'}\n",
    "target_dict['sponsor_termination'] = {'target': 'sponsor_termination', 'model': 'lr', 'metric': 'f1', 'model_input': 'SentenceLemmas'}\n",
    "target_dict['mezzanine_outstanding'] = {'target': 'mezzanine_outstanding', 'model': 'lr', 'metric': 'f1', 'model_input': 'SentenceLemmas'}\n",
    "target_dict['aggregate_dscr_fall'] = {'target': 'aggregate_dscr_fall', 'model': 'lr', 'metric': 'f1', 'model_input': 'SentenceLemmas'}\n",
    "target_dict['sff'] = {'target': 'sff', 'model': 'lr', 'metric': 'f1', 'model_input': 'SentenceLemmas'}\n",
    "\n",
    "# set output_dict - will contain target + output of calculations\n",
    "output_dict = {}\n",
    "\n",
    "# get data\n",
    "raw_df = get_data()\n",
    "raw_df = clean_data(raw_df)\n",
    "\n",
    "# Reshape our original dataframe and tokenize to prepare sentences for models\n",
    "df = reshape_trigger_representation(raw_df)\n",
    "df = tokenize_sentences(df)\n",
    "\n",
    "# run for each model definition\n",
    "for k,v in target_dict.items():\n",
    "    \n",
    "    print(f\"{target_dict[k]['target'].replace('_', ' ').title()}: creating Train-Test split\")\n",
    "    # get split data\n",
    "    split_data = get_split_data(df, v)\n",
    "    \n",
    "    print(f\"Model fit in progress: {target_dict[k]}\")\n",
    "    # run model\n",
    "    model_result = run_model(split_data, v)\n",
    "    \n",
    "    # make the output dictionary\n",
    "    output_dict[k] = v\n",
    "    output_dict[k]['split_data'] = split_data\n",
    "    output_dict[k]['model_result'] = model_result\n",
    "\n",
    "    \n",
    "    print(f\"Best fit parameters: {model_result.best_params_}\")\n",
    "    print(f\"Best fit 3-fold cross validation score: {model_result.best_score_:.3f}\")\n",
    "    print(f\"{target_dict[k]['target'].replace('_', ' ').title()} {target_dict[k]['metric']} Train score: {model_result.score(split_data['X_train'], split_data['y_train']):.3f}\")\n",
    "    print(f\"{target_dict[k]['target'].replace('_', ' ').title()} {target_dict[k]['metric']} Test score: {model_result.score(split_data['X_test'], split_data['y_test']):.3f}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    output_dict[k]['best_params'] = model_result.best_params_\n",
    "    output_dict[k]['count'] = df[target_dict[k]['target']].value_counts()[1]\n",
    "    output_dict[k]['best_crossval_score'] = model_result.best_score_\n",
    "    output_dict[k]['train_score'] = model_result.score(split_data['X_train'], split_data['y_train'])\n",
    "    output_dict[k]['test_score'] = model_result.score(split_data['X_test'], split_data['y_test'])                                                  \n",
    "    \n",
    "\n",
    "full_output_dict = [output_dict[key] for key in output_dict.keys()]\n",
    "results_df = pd.DataFrame.from_dict(full_output_dict)\n",
    "    \n",
    "print(\"Modeling complete!\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a summary table with all fitting results\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export summary table\n",
    "results_df.to_csv(f'../data/exported_data/results_df_06_09_20.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting functions: features and confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coefficient_plots(results_df):\n",
    "    '''\n",
    "    Plots the top 10 and bottom 10 coefficients for each category\n",
    "    '''\n",
    "    for row in results_df.itertuples(index = True, name = 'Pandas'):\n",
    "        names = getattr(row, \"model_result\").best_estimator_.steps[0][1].get_feature_names()\n",
    "        betas = getattr(row, \"model_result\").best_estimator_.named_steps['lr']\n",
    "        coef = np.array(betas.coef_).tolist()[0]\n",
    "        top_words = list(zip(names, coef))\n",
    "        df_coefs = pd.DataFrame(top_words).rename(columns = {0: 'word', 1: 'coef'}).sort_values(by = 'coef', ascending = True)\n",
    "        \n",
    "\n",
    "        coefs_1 = df_coefs.sort_values(by = 'coef', ascending=False).tail(10) # getting the top 10 coefficients\n",
    "        coefs_2 = df_coefs.sort_values(by = 'coef', ascending=False).head(10) # getting the bottom 10 coefficients\n",
    "        coefs = pd.concat([coefs_2, coefs_1], axis = 0) # merging the two into one\n",
    "        # plotting importance\n",
    "        plt.figure(figsize=(10, 8)) # plotting the coefficients\n",
    "        plt.title(f'Feature Coefficients for {getattr(row,\"target\").replace(\"_\", \" \").title()}', fontsize=25)\n",
    "        sns.set_style(\"darkgrid\")\n",
    "        sns.barplot(data=coefs,\n",
    "                    x= 'coef',\n",
    "                    y='word',\n",
    "                    orient='h',\n",
    "                    palette = 'PuBuGn_d')\n",
    "        plt.xlabel('coefficient', fontsize=15)\n",
    "        plt.ylabel('feature', fontsize=15)\n",
    "        plt.tick_params(labelsize=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the coefficient plot function\n",
    "coefficient_plots(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_confusion_matrix(results_df):\n",
    "    '''\n",
    "    Plots the top 10 and bottom 10 coefficients for each category\n",
    "    '''\n",
    "    for row in results_df.itertuples(index = True, name = 'Pandas'):\n",
    "        best_model = getattr(row, \"model_result\").best_estimator_\n",
    "        preds = best_model.predict(getattr(row, \"split_data\")['X_test'])\n",
    "        \n",
    "        cm_test = confusion_matrix(getattr(row, \"split_data\")['y_test'], preds)\n",
    "        cm_test = pd.DataFrame(cm_test, columns=['Predicted Negative','Predicted Positive'], \n",
    "                       index=['Actual Negative','Actual Positive'])\n",
    "        plt.figure(figsize = (6,6))\n",
    "        ax = plt.subplot()\n",
    "        sns.heatmap(cm_test, \n",
    "            annot=True, \n",
    "            ax = ax, \n",
    "            fmt='g', \n",
    "            cbar=False,\n",
    "            cmap=\"Blues\"); #annot=True to annotate cells\n",
    "\n",
    "        # labels, title and ticks\n",
    "        ax.set_title(f'Confusion Matrix - {getattr(row,\"target\").replace(\"_\", \" \").title()}', size=16)\n",
    "        ax.set_xlabel('Predicted', size=14)\n",
    "        ax.set_ylabel('Actual', size=14)\n",
    "        ax.xaxis.set_ticklabels(['Negative', 'Positive'])\n",
    "        ax.yaxis.set_ticklabels(['Negative', 'Positive']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_confusion_matrix(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To allow us to better read the Sentences\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_result_table(results_df):\n",
    "    \n",
    "    global final_misclassified_as_true\n",
    "    global final_misclassified_as_false\n",
    "    \n",
    "    target_list = results_df['target'].values.tolist()\n",
    "    final_columns = ['prediction', 'actual'] + target_list\n",
    "    \n",
    "    final_misclassified_as_true = pd.DataFrame(columns = final_columns)\n",
    "    final_misclassified_as_false = pd.DataFrame(columns = final_columns)\n",
    "\n",
    "    for row in results_df.itertuples(index = True, name = 'Pandas'):\n",
    "        best_model = getattr(row, \"model_result\").best_estimator_\n",
    "        preds = best_model.predict(getattr(row, \"split_data\")['X_test'])\n",
    "        \n",
    "        index_list = list(getattr(row, \"split_data\")['indices_test'])\n",
    "        preds_list = list(preds.astype(int))\n",
    "        actuals_list = list(getattr(row, \"split_data\")['y_test'].astype(int))\n",
    "        \n",
    "        results = pd.DataFrame({'index': index_list, 'prediction': preds_list, 'actual': actuals_list})\n",
    "        results.set_index('index', inplace = True)\n",
    "        \n",
    "        misclassified = results[results['prediction'] != results['actual']]\n",
    "        misclassified = misclassified.merge(df, how = 'left', left_index = True, right_index = True)\n",
    "        misclassified_as_true = misclassified.loc[misclassified['prediction'] == 1]\n",
    "        misclassified_as_false = misclassified.loc[misclassified['prediction'] == 0]\n",
    "        \n",
    "        misclassified_as_true_summary = misclassified_as_true.drop(columns = ['Document', 'Sentence', 'SentenceTokens', 'SentenceLemmas'])\n",
    "        misclassified_as_false_summary = misclassified_as_false.drop(columns = ['Document', 'Sentence', 'SentenceTokens', 'SentenceLemmas'])\n",
    "        \n",
    "        \n",
    "        misclassified_as_true_summary.loc[f'{getattr(row, \"target\")}',:] = misclassified_as_true.sum(axis = 0).copy(deep = True)\n",
    "        misclassified_as_false_summary.loc[f'{getattr(row, \"target\")}',:] = misclassified_as_false.sum(axis = 0).copy(deep = True)\n",
    "\n",
    "        \n",
    "        final_misclassified_as_true = final_misclassified_as_true.append(misclassified_as_true_summary.iloc[-1], ignore_index = False)\n",
    "        final_misclassified_as_false = final_misclassified_as_false.append(misclassified_as_false_summary.iloc[-1], ignore_index = False)\n",
    "        \n",
    "    return final_misclassified_as_true\n",
    "    return final_misclassified_as_false\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_summary_result_table(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.set_index('target', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_misclassified_as_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_misclassified_as_false = results_df[['count', 'train_score', 'test_score']].merge(final_misclassified_as_false, left_index = True, right_index = True)\n",
    "final_misclassified_as_false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_misclassified_as_true = results_df[['count', 'train_score', 'test_score']].merge(final_misclassified_as_true, left_index = True, right_index = True)\n",
    "final_misclassified_as_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV file\n",
    "\n",
    "final_misclassified_as_false_summary.to_csv(f'../data/exported_data/misclassified_as_false_06_09_20.csv')\n",
    "final_misclassified_as_true_summary.to_csv(f'../data/exported_data/misclassified_as_true_06_09_20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "415.764px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
